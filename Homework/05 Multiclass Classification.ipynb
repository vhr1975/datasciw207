{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Lab 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43534tdfgs-v"
   },
   "source": [
    "This lab extends binary logistic regression to [multi-class logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression), which goes by a variety of names, including *softmax regression*, due to the use of the softmax function, which generalizes the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3490,
     "status": "ok",
     "timestamp": 1620709486769,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 420
    },
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll use below.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4uq43KrCh8pe"
   },
   "source": [
    "We again load the Fashion MNIST dataset, though this time around, we'll use all 10 output classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1905,
     "status": "ok",
     "timestamp": 1618441063929,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 420
    },
    "id": "load_auto_data_set_code",
    "outputId": "36d947cb-8799-4e55-d208-fbc7350c0cc2"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# Load the Fashion MNIST dataset.\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "# Flatten Y_train and Y_test, so they become vectors of label values.\n",
    "# The label for X_train[0] is in Y_train[0].\n",
    "Y_train = Y_train.flatten()\n",
    "Y_test = Y_test.flatten()\n",
    "\n",
    "label_names = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "\n",
    "# Apply random shufflying to training examples.\n",
    "np.random.seed(0)\n",
    "indices = np.arange(X_train.shape[0])\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "X_train = X_train[shuffled_indices]\n",
    "Y_train = Y_train[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsfzNnVFwh0K"
   },
   "source": [
    "### Cross-Entropy Loss\n",
    "\n",
    "Recall the log loss function (also called binary cross-entropy):\n",
    "\n",
    "\\begin{equation}\n",
    "-y log(\\hat{y}) + (1−y)log(1−\\hat{y})\n",
    "\\end{equation}\n",
    "\n",
    "In the above formulation, it is assumed that $y$ is either 0 or 1, so either the left term or the right term is active for each example.\n",
    "\n",
    "The general form for cross-entropy is used when $y$ is assumed to be a label vector with a 1 in the index of the true label and a 0 everywhere else: $y=[0,0,0,0,0,0,0,1,0,0]$ implies a label of \"sneaker\" in this dataset (the 7th label). Accordingly, $\\hat{y}$ is a vector of predicted probabilities. Then the cross-entropy loss is simply:\n",
    "\n",
    "\\begin{equation}\n",
    "-\\sum_{j} y_j log(\\hat{y}_j)\n",
    "\\end{equation}\n",
    "\n",
    "As in the binary case, this summation will have exactly 1 non-zero term where the true label $y_j=1$.\n",
    "\n",
    "Note that this formulation is using a *dense* representation of the label. The corresponding *sparse* representation would use the non-zero index directly ($y=7$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09EpBz1w0_Nj"
   },
   "source": [
    "## Build a model\n",
    "\n",
    "Let's construct a model much like we did in the binary classification case, but now with a multi-class output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "75ur9Q_TOoxv"
   },
   "source": [
    "---\n",
    "### Exercise 1 (8 points)\n",
    "\n",
    "Fill in the NotImplemented parts of the build_model function below. You will need to make the following changes to generalize the binary case to the multi-class case:\n",
    "* The output will include 10 probabilities instead of 1.\n",
    "* Use a softmax function instead of a sigmoid.\n",
    "* Use a [sparse_categorical_crossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/sparse_categorical_crossentropy) loss instead of binary_crossentropy. Note that \"sparse\" refers to the use of a sparse index (e.g. 7) to indicate the label rather than a dense vector (e.g. [0,0,0,0,0,0,0,1,0,0]).\n",
    "\n",
    "Check that training works below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GeBGPGhQ5bu"
   },
   "outputs": [],
   "source": [
    "def build_model(n_classes, learning_rate=0.01):\n",
    "  \"\"\"Build a multi-class logistic regression model using Keras.\n",
    "\n",
    "  Args:\n",
    "    n_classes: Number of classes in the dataset\n",
    "    learning_rate: The desired learning rate for SGD.\n",
    "\n",
    "  Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "  \"\"\"\n",
    "  tf.keras.backend.clear_session()\n",
    "  np.random.seed(0)\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = keras.Sequential()\n",
    "  model.add(keras.layers.Flatten())\n",
    "  model.add(keras.layers.Dense(\n",
    "      # YOUR CODE HERE\n",
    "      units=NotImplemented,\n",
    "      activation=NotImplemented\n",
    "  ))\n",
    "\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "    \n",
    "  # YOUR CODE HERE\n",
    "  model.compile(loss=NotImplemented, \n",
    "                optimizer=optimizer, \n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO-d_F58Q-6O"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNpKsUK6jt7-"
   },
   "source": [
    "Make sure your model trains. If you configured it properly, the training loss should get to ~0.55 after 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 394
    },
    "executionInfo": {
     "elapsed": 7672,
     "status": "ok",
     "timestamp": 1618441373961,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 420
    },
    "id": "dYRGXpzY1B5x",
    "outputId": "2b30c3ca-cfca-4160-86a9-5026697d685e"
   },
   "outputs": [],
   "source": [
    "model = build_model(len(label_names), 0.01)\n",
    "\n",
    "history = model.fit(\n",
    "  x = X_train,\n",
    "  y = Y_train,\n",
    "  epochs=5,\n",
    "  batch_size=64,\n",
    "  validation_split=0.1,\n",
    "  verbose=1)\n",
    "\n",
    "history = pd.DataFrame(history.history)\n",
    "display(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ALSrErIrrjW"
   },
   "source": [
    "### Evaluation Metrics\n",
    "\n",
    "Classification metrics like accuracy, precision, and recall can all be derived from a confusion matrix which displays the counts for all pairs of true label and predicted label. Correct predictions are on the diagonal and incorrect predictions (confusions) are off the diagonal.\n",
    "\n",
    "First, we need the predicted labels from the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8936,
     "status": "ok",
     "timestamp": 1616042832272,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 420
    },
    "id": "eBdgn9PUsHuq",
    "outputId": "4744e0bb-e67f-413c-ba24-ae7d65d9c69e"
   },
   "outputs": [],
   "source": [
    "# Recall that model.predict gives a vector of probabilities for each x.\n",
    "# Get labels by taking the argmax -- the index with the largest probability.\n",
    "test_predictions = np.argmax(model.predict(X_test), axis=-1)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr8NOrQmfwsT"
   },
   "source": [
    "Next, we create a confusion matrix and produce an easy-to-read visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "executionInfo": {
     "elapsed": 9529,
     "status": "ok",
     "timestamp": 1616042832868,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 420
    },
    "id": "juOwp33Wx_bR",
    "outputId": "5565824a-cf5f-450f-d54d-faf99c613c70"
   },
   "outputs": [],
   "source": [
    "# Create a confusion matrix as a 2D array.\n",
    "confusion_matrix = tf.math.confusion_matrix(Y_test, test_predictions)\n",
    "\n",
    "# Use a heatmap plot to display it.\n",
    "ax = sns.heatmap(confusion_matrix, annot=True, fmt='.3g', cmap='Blues',\n",
    "                 xticklabels=label_names, yticklabels=label_names, cbar=False)\n",
    "\n",
    "# Add axis labels.\n",
    "ax.set(xlabel='Predicted Label', ylabel='True Label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k-BrE8eaEw60"
   },
   "source": [
    "---\n",
    "### Exercise 2 (8 points)\n",
    "\n",
    "Answer the following questions by studying the confusion matrix (above).\n",
    "\n",
    "1. Which class has the lowest precision? What is that precision? Which class has the highest precision? What is that precision?\n",
    "\n",
    "2. For the class 'sneaker', what class is the largest source of false negatives? What about the largest source of false positives?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hfQfsvzpVwfK"
   },
   "source": [
    "*Written answer:*\n",
    "\n",
    "1. \n",
    "\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpFmB9v5V4CE"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epIqYiQhQpJC"
   },
   "source": [
    "## Analyze Errors\n",
    "\n",
    "A crucial part of the development cycle in Machine Learning is analyzing errors to help understand the shortcomings of the model. While typically we'd want to use the development data for this purpose to preserve the purity of the test set, we'll just use our test split for simplicity.\n",
    "\n",
    "Since the \"shirt\" class seems to be the source for a lot of errors, let's look at some of the confusions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLK2zFv2SWAy"
   },
   "source": [
    "---\n",
    "### Exercise 3 (8 points)\n",
    "\n",
    "1. Display 5 images with true label \"shirt\", but predicted label \"coat\" (false negatives for the \"shirt\" class).\n",
    "2. Display 5 images with predicted label \"shirt\", but true label \"coat\" (false positives for the \"shirt\" class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUZfCG1ylVtp"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCmcQmKXlYK7"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyMHf5pGjkxm"
   },
   "source": [
    "## Visualize Parameters\n",
    "Finally, here's some code that helps visualize the learned parameters for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 408
    },
    "executionInfo": {
     "elapsed": 12395,
     "status": "ok",
     "timestamp": 1616042835746,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 420
    },
    "id": "C-6ovDo8jvLH",
    "outputId": "9c2b3eac-bd32-4d3f-9e6c-3eade55f272f"
   },
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()\n",
    "fig, axs = plt.subplots(2, 5, figsize=(16,7))\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "  ax.imshow(weights[:,i].reshape(28,28), cmap='PRGn')\n",
    "  ax.axis('off')\n",
    "  ax.set_title(label_names[i])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN9qpWEqErz0SpDHvJfddfe",
   "name": "05 Multiclass Classification.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
