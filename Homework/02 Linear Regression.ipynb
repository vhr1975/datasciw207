{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHLcriKWLRe4"
   },
   "source": [
    "# Lab 2\n",
    "Let's start with the same artificial data we used in Lab 1. Remember that we considered 2 models:\n",
    "1. $M_1(x) = x+5$ \n",
    "2. $M_2(x) = 2x+1$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll use below.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ulmn_bFdU87t"
   },
   "outputs": [],
   "source": [
    "def create_1d_data(num_examples=10, w=2, b=1, random_scale=1):\n",
    "  \"\"\"Create X, Y data with a linear relationship with added noise.\n",
    "\n",
    "  Args:\n",
    "    num_examples: number of examples to generate\n",
    "    w: desired slope\n",
    "    b: desired intercept\n",
    "    random_scale: add uniform noise between -random_scale and +random_scale\n",
    "\n",
    "  Returns:\n",
    "    X and Y with shape (num_examples)\n",
    "  \"\"\"\n",
    "  X = np.arange(num_examples)\n",
    "  np.random.seed(4)  # consistent random number generation\n",
    "  deltas = np.random.uniform(low=-random_scale, high=random_scale, size=X.shape)\n",
    "  Y = b + deltas + w * X\n",
    "  return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qJg0IiYVJ8U"
   },
   "outputs": [],
   "source": [
    "# Create some artificial data using create_1d_data.\n",
    "X, Y = create_1d_data()\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCWuhbfLNmna"
   },
   "source": [
    "## Notation\n",
    "In our artificial data, things are pretty simple: each input example is just a single value. But soon, each input example will include multiple values or *features*, so we need some conventions to avoid confusion.\n",
    "\n",
    "Let's start with the inputs:\n",
    "\n",
    "\\begin{align}\n",
    "X =\n",
    "\\begin{pmatrix}\n",
    "x^{(0)} \\\\\n",
    "x^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "x^{(m-1)}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "* Capital $X$ refers to all input examples together.\n",
    "* Lowercase $x$ refers to an individual input example; we use $x^{(i)}$ to refer to input example $i$; there are $m$ total examples.\n",
    "\n",
    "Further, each input example $x$ could itself be a vector of feature values:\n",
    "\n",
    "\\begin{align}\n",
    "x = [x_0, x_1, \\dots x_{n-1}]\n",
    "\\end{align}\n",
    "\n",
    "* Lowercase $x$ refers to all input features together for an individual input example.\n",
    "* $x_i$ refers to feature $i$ for an input example $x$; there are $n$ total features.\n",
    "\n",
    "Similarly, we can index labels $y^{(i)}$ in $Y$, which we can think of as a column vector where $y^{(i)}$ is the label for $x^{(i)}$.\n",
    "\n",
    "\\begin{align}\n",
    "Y =\n",
    "\\begin{pmatrix}\n",
    "y^{(0)} \\\\\n",
    "y^{(1)} \\\\\n",
    "\\vdots \\\\\n",
    "y^{(m-1)}\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In general, we're using matrix notation. Rows refer to examples and columns refer to features. If we want to be very specific and refer to a particular feature of a particular input example, we can use $x_{i,j}$ for input $i$, feature $j$. Using matrices will be useful for coding ML algorithms since most of the operations we will do can be expressed as operations on matrices.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2szkkNDvsCfn"
   },
   "source": [
    "##Parameter Vectors\n",
    "Let's prepare to learn a linear model $h(x)$ that approximates values of $Y$ from corresponding values of $X$. Since our input data has only one feature, our model will have two parameters (also called weights), which we'll refer to collectively as $W$:\n",
    "\n",
    "\\begin{align}\n",
    "h(x) = w_0 + w_1x\n",
    "\\end{align}\n",
    "\n",
    "Notice that if we prepend an extra feature (column) to $X$ that is always $1$, we can rewrite our model using a matrix multiplication:\n",
    "\n",
    "\\begin{align}\n",
    "h(x) = w_0x_0 + w_1x_1 = xW^T\n",
    "\\end{align}\n",
    "\n",
    "To make this matrix formulation as clear as possible, this is:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{y} = xW^T =\n",
    "\\begin{pmatrix}\n",
    "x_0 & x_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "In addition, if we wanted to apply our model to *all* inputs $X$, we could simply use $XW^T$:\n",
    "\n",
    "\\begin{align}\n",
    "\\hat{Y} = XW^T =\n",
    "\\begin{pmatrix}\n",
    "x_{0,0} & x_{0,1} \\\\\n",
    "x_{1,0} & x_{1,1} \\\\\n",
    "\\vdots & \\vdots \\\\\n",
    "x_{m-1,0} & x_{m-1,1} \\\\\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "w_0 \\\\\n",
    "w_1 \\\\\n",
    "\\end{pmatrix}\n",
    "\\end{align}\n",
    "\n",
    "Remember that [matrix multiplication](https://en.wikipedia.org/wiki/Matrix_multiplication) requires the inner dimensions to line up: \n",
    "\n",
    "\\begin{align}\n",
    "X_{\\{m \\times n\\}} W^T_{\\{n \\times 1 \\}}  = \\hat{Y}_{\\{m \\times 1 \\}}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8NXo1n9j1LMT"
   },
   "source": [
    "### Exercise 1: Practice with Parameters\n",
    "Add a column of 1s to $X$. Then, use matrix multiplication (np.dot) to apply $M_1$ and $M_2$ (from above) to produce vectors of predictions. Print the shapes of the predictions to validate that they have the same shape as $Y$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0FIiQ-j6tgn"
   },
   "source": [
    "#### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aBEZ_QOX6qOi"
   },
   "outputs": [],
   "source": [
    "# Add a column of 1s to X by using np.c_ to concatenate with the current values.\n",
    "X_with_1s = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LUNVK2acFMQ0"
   },
   "source": [
    "## Gradient Descent\n",
    "Here we'll demonstrate gradient descent for linear regression to learn the weight vector $W$. We'll use the more specific notation $h_W(x)$ since we want to specify that $h$ is parameterized by $W$. As above, we'll assume that $x_0=1$ so we can write $h$ as a sum or a matrix product:\n",
    "\n",
    "\\begin{align}\n",
    "h_W(x) = \\sum_{i=0}^{n-1} w_i x_i = x W^T\n",
    "\\end{align}\n",
    "\n",
    "In the derivation that follows, we'll use summations, but in the code below, we'll use matrix computations.\n",
    "\n",
    "In linear regression, we compute the loss, $J(W)$ from the mean squared difference between predictions $h_W(x)$ and targets $y$. In the following equation, we average the loss over each of the $m$ training examples.\n",
    "\n",
    "\\begin{align}\n",
    "J(W) = \\frac{1}{2m} \\sum_{i=0}^{m-1} (h_W(x^{(i)}) - y^{(i)})^2\n",
    "\\end{align}\n",
    "\n",
    "Dividing by $2$ simplifies the formula of the gradient, since it cancels out the constant $2$ from by the derivative of the squared term (see below). Remember that the gradient is a vector of partial derivatives for each $w_j$ (holding the other elements of $w$ constant). The gradient points in direction of steepest ascent for the loss function $J$.\n",
    "\n",
    "Here we derive the parameter update rule by computing the gradient of the loss function. We need a derivative for each feature in $x$, so we'll show how to compute the derivative with respect to $w_j$. For simplicity, let's assume we have only one training example ($m = 1$):\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{\\partial}{\\partial w_j} J(W) &= \\frac{\\partial}{\\partial w_j} \\frac{1}{2} (h_W(x) - y)^2 \\tag{1}\\\\\n",
    "&= 2 \\cdot \\frac{1}{2} (h_W(x) - y) \\cdot \\frac{\\partial}{\\partial w_j} (h_W(x) - y) \\tag{2}\\\\\n",
    "&= (h_W(x) - y) \\frac{\\partial}{\\partial w_j} \\left(\\sum_{i=0}^{n-1} w_i x_i - y \\right) \\tag{3}\\\\\n",
    "&= (h_W(x) - y)x_j \\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "The derivation has 2 key steps:\n",
    "\n",
    "(1) Apply the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) (step 1 -> 2).\n",
    "\n",
    "(2) The derivative with respect to $w_j$ of $h_W(x)$ is only non-zero for $w_j x_j$. For this component, the derivative is $x_j$ since the feature value is treated as a constant (step 3 -> 4).\n",
    "\n",
    "Ok, that's it. We can now implement gradient descent for linear regression. The only difference in the code below is that it computes the loss as an average over all training examples (rather than just a single example)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaXYiTm9ftRf"
   },
   "source": [
    "### Exercise 2: Implementing Gradient Descent for Linear Regression\n",
    "Fill in the `NotImplemented` parts of the gradient descent function below. There are detailed comments to help guide you. Note that this function uses vectors and matrices so you'll want to use numpy functions like `np.dot` to multiply them, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gTlTUJkS4DzQ"
   },
   "source": [
    "##### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_hP9rzDyFXTg"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(inputs, outputs, learning_rate, num_epochs):\n",
    "  \"\"\"Apply the gradient descent algorithm to learn learn linear regression.\n",
    "\n",
    "  Args:\n",
    "    inputs: A 2-D array where each column is an input feature and each\n",
    "            row is a training example.\n",
    "    outputs: A 1-D array containing the real-valued\n",
    "             label corresponding to the input data in the same row.\n",
    "    learning_rate: The learning rate to use for updates.\n",
    "    num_epochs: The number of passes through the full training data.\n",
    "\n",
    "  Returns:\n",
    "    weights: A 2-D array with the learned weights after each training epoch.\n",
    "    losses: A 1-D array with the loss after each epoch.\n",
    "  \"\"\"\n",
    "  # m = number of examples, n = number of features\n",
    "  m, n = inputs.shape\n",
    "  \n",
    "  # We'll use a vector of size n to store the learned weights and initialize\n",
    "  # all weights to 1. \n",
    "  W = np.ones(n)\n",
    "  \n",
    "  # Keep track of the training loss and weights after each step.\n",
    "  losses = []\n",
    "  weights = []\n",
    "  \n",
    "  for epoch in range(num_epochs):\n",
    "    # Append the old weights to the weights list to keep track of them.\n",
    "    weights.append(W)\n",
    "\n",
    "    # Evaluate the current predictions for the training examples given\n",
    "    # the current estimate of W (you did this in exercise 1). \n",
    "    predictions = NotImplemented\n",
    "    \n",
    "    # Find the difference between the predictions and the actual target\n",
    "    # values.\n",
    "    diff = NotImplemented\n",
    "    \n",
    "    # In standard linear regression, we want to minimize the sum of squared\n",
    "    # differences. Compute the mean squared error loss. Don't bother with the\n",
    "    # 1/2 scaling factor here.\n",
    "    loss = NotImplemented\n",
    "\n",
    "    # Append the loss to the losses list to keep a track of it.\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # Compute the gradient with respect to the loss.\n",
    "    # [Formula (4) in the Gradient Descent Implementation]\n",
    "    gradient = NotImplemented\n",
    "\n",
    "    # Update weights, scaling the gradient by the learning rate.\n",
    "    W = W - learning_rate * gradient\n",
    "      \n",
    "  return np.array(weights), np.array(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXN_YY-daSPK"
   },
   "source": [
    "Let's try running gradient descent with our artificial data and print out the results. Note that we're passing the version of the input data with a column of $1s$ so that we learn an *intercept* (also called a *bias*). We can also try learning without the intercept.\n",
    "\n",
    "Note: if your implementation of gradient descent is correct, you should get a loss of ~0.409 after 5 epochs (with a bias parameter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4z23bKHayGU"
   },
   "outputs": [],
   "source": [
    "print('Running gradient descent...')\n",
    "weights, losses = gradient_descent(X_with_1s, Y, learning_rate=.02,\n",
    "                                   num_epochs=5)\n",
    "for W, loss in zip(weights, losses):\n",
    "  print(loss, W)\n",
    "\n",
    "print('\\nRunning gradient descent without biases...')\n",
    "# Make sure we're providing an input with the right 2-D shape.\n",
    "X_without_1s = np.expand_dims(X, axis=0).T\n",
    "weights_without_bias, losses_without_bias = gradient_descent(X_without_1s, Y,\n",
    "                                                             .02, num_epochs=5)\n",
    "for W, loss in zip(weights_without_bias, losses_without_bias):\n",
    "  print(loss, W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V7vGx68Ec2Si"
   },
   "source": [
    "### Exercise 3: Interpreting the Model\n",
    "Write down the learned model with and without an intercept term. Which model fits the data better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZlVWFCRtqZIZ"
   },
   "source": [
    "#### Student Solution\n",
    "\n",
    "WRITE YOUR ANSWERS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-DdmWmPrDvk"
   },
   "source": [
    "## Gradient Descent Progress\n",
    "Let's write a function that lets us visualize the progress of gradient descent during training. Our gradient descent function already provides intermediate weight vectors and losses after each epoch, so we just need to plot these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lWcTeR_gNz9d"
   },
   "outputs": [],
   "source": [
    "def plot_learning(inputs, outputs, weights, losses):\n",
    "  \"\"\"Plot predictions and losses after each training epoch.\n",
    "\n",
    "  Args:\n",
    "    inputs: A 2-D array where each column is an input feature and each\n",
    "            row is a training example.\n",
    "    outputs: A 1-D array containing the real-valued\n",
    "             label corresponding to the input data in the same row.\n",
    "    weights: A 2-D array with the learned weights after each training epoch.\n",
    "    losses: A 1-D array with the loss after each epoch.\n",
    "  \"\"\"\n",
    "  # Create a figure.\n",
    "  plt.figure(1, figsize=[10,4])\n",
    "\n",
    "  # The first subplot will contain the predictions. Start by plotting the\n",
    "  # outputs (Y).\n",
    "  plt.subplot(121)\n",
    "  plt.xlabel('x')\n",
    "  plt.ylabel('y')\n",
    "  plt.xticks(inputs[:,1])\n",
    "  plt.scatter(inputs[:,1], outputs, color='black', label='Y')\n",
    "  \n",
    "  # For each epoch, retrieve the estimated weights W, compute predictions, and\n",
    "  # plot the resulting line.\n",
    "  num_epochs = len(weights)\n",
    "  for i in range(num_epochs):\n",
    "    W = weights[i]\n",
    "    predictions = np.dot(inputs, W.T)\n",
    "    plt.plot(inputs[:,1], predictions, label='Epoch %d' %i)\n",
    "  plt.legend()\n",
    "\n",
    "  # The second subplot will contain the losses.\n",
    "  plt.subplot(122)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xticks(range(num_epochs))\n",
    "  plt.plot(range(num_epochs), losses, marker='o', color='black',\n",
    "           linestyle='dashed')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WECpzvgkrkFk"
   },
   "source": [
    "### Exercise 4: Plotting Progress\n",
    "\n",
    "Re-run gradient descent using X_with_1s, but this time with learning_rate=0.01 and num_epochs=7.\n",
    "\n",
    "Run the plot_learning function using the weights and losses returned by gradient_descent (from above) and answer the following questions:\n",
    "\n",
    "1. Is learning converging faster or slower than when we used learning_rate=0.02?\n",
    "2. If you continue training, will the loss eventually reach 0?\n",
    "3. If you continue training, will the model eventually converge to $h(x)=2x+1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OjfwzNhzrB1D"
   },
   "source": [
    "#### Student Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7CK6106tWgi"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-mgY_e5upTL"
   },
   "source": [
    "WRITE YOUR ANSWERS HERE\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bzeVjc2Vp7y"
   },
   "source": [
    "## Review\n",
    "* We store our data in arrays where **each row is an input example** $x^{(i)}$ and **each column is a feature**. Training example $x^{(i)}$ corresponds to training label $y^{(i)}$.\n",
    "* **Gradient descent** is an **optimization process** that **minimizes loss** $J(W)$ where $W$ is a set of parameters (or weights). The loss measures the difference between  predictions $\\hat{Y}$ using the current values of $W$ and the target labels $Y$, and gradient descent updates $W$ by taking a **step in the direction of the loss gradient**.\n",
    "* Each pass over the training data by the gradient descent algorithm is called an **epoch**. The algorithm has no specific stopping point, but we often choose to stop when the parameter values have **converged**, that is, the change in values in the next step are less than some small $\\epsilon$."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "02 Linear Regression.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
