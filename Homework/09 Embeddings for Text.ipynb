{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Lab 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43534tdfgs-v"
   },
   "source": [
    "In this lab, we'll train models for sentiment classification and experiment with learned embeddings for text features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll use below.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "import plotly.graph_objs as plotly  # for interactive plots\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqppUDpmdptk"
   },
   "source": [
    "## Data for Sentiment Classification\n",
    "\n",
    "In this lab, we'll train a *sentiment* classifier for movie reviews. That is, the input is the text of a movie review and the output is the probability the input was a positive review. The target labels are binary, 0 for negative and 1 for positive.\n",
    "\n",
    "Our data includes 50,000 movie reviews on IMDB. The data comes pre-segmented into train and test splits. The [data loading function](https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data) below also splits each input text into tokens (words) and maps the words to integer values. Each input is a sequence of integers corresponding to the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5870,
     "status": "ok",
     "timestamp": 1646684495083,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "s6M-asvhQWV_",
    "outputId": "1aca520b-e1b6-4006-ac44-78ff33ac6da9"
   },
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = imdb.load_data(path=\"imdb.npz\",\n",
    "                                                      num_words=None,\n",
    "                                                      skip_top=0,\n",
    "                                                      maxlen=None,\n",
    "                                                      seed=113,\n",
    "                                                      start_char=1,\n",
    "                                                      oov_char=2,\n",
    "                                                      index_from=3)\n",
    "\n",
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"Y_train.shape:\", Y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"Y_test.shape:\", Y_test.shape)\n",
    "\n",
    "print('First training example data:', X_train[0])\n",
    "print('First training example label:', Y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MyIWiy-4gQK-"
   },
   "source": [
    "So our first training example is a positive review. But that sequence of integer IDs is hard to read. The data loader provides a dictionary mapping words to IDs. Let's create a reverse index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 801,
     "status": "ok",
     "timestamp": 1646684508506,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "HQ-qATkhUj7c",
    "outputId": "eea86a69-fe6a-4cdb-ac8c-9307a5118e47"
   },
   "outputs": [],
   "source": [
    "# The imdb dataset comes with an index mapping words to integers.\n",
    "# In the index the words are ordered by frequency they occur.\n",
    "index = imdb.get_word_index()\n",
    "\n",
    "# Because we used index_from=3 (above), setting aside ids below 3 for special\n",
    "# symbols, we need to add 3 to the index values.\n",
    "index = dict([(key, value+3) for (key, value) in index.items()])\n",
    "\n",
    "# Create a reverse index so we can lookup tokens assigned to each id.\n",
    "reverse_index = dict([(value, key) for (key, value) in index.items()])\n",
    "reverse_index[1] = '<START>'  # start of input\n",
    "reverse_index[2] = '#'        # out-of-vocabulary (OOV)\n",
    "reverse_index[3] = '<UNUSED>'\n",
    "\n",
    "max_id = max(reverse_index.keys())\n",
    "print('Largest ID:', max_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h76-b07ehWNQ"
   },
   "source": [
    "Note that our index (and reverse index) have over 88,000 tokens. That's quite a large vocabulary! Let's also write a decoding function for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 358,
     "status": "ok",
     "timestamp": 1646684531998,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "UjobmouHS5Dm",
    "outputId": "29975a48-7fda-4600-bd0c-693c35dd8a54"
   },
   "outputs": [],
   "source": [
    "def decode(token_ids):\n",
    "  \"\"\"Return a string with the decoded text given a list of token ids.\"\"\"\n",
    "  # Try looking up each id in the index, but return '#' (for OOV) if not found.\n",
    "  tokens = [reverse_index.get(i, \"#\") for i in token_ids]\n",
    "\n",
    "  # Connect the string tokens with a space.\n",
    "  return ' '.join(tokens)\n",
    "\n",
    "# Show the ids corresponding tokens in the first example.\n",
    "print(X_train[0])\n",
    "print(decode(X_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g47w5CackGBA"
   },
   "source": [
    "### Text Lengths\n",
    "As usual, let's start with some data analysis. How long are the reviews? Is there a difference in length between positive and negative reviews? A histogram will help answer these questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 305
    },
    "executionInfo": {
     "elapsed": 762,
     "status": "ok",
     "timestamp": 1646684547021,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "kEOgzo8Gk3r7",
    "outputId": "e7446ffd-10c5-4c64-a5bc-7cab81a1d73f"
   },
   "outputs": [],
   "source": [
    "# Create a list of lengths for training examples with a positive label.\n",
    "text_lengths_pos = [len(x) for (i, x) in enumerate(X_train) if Y_train[i]]\n",
    "\n",
    "# And a list of lengths for training examples with a negative label.\n",
    "text_lengths_neg = [len(x) for (i, x) in enumerate(X_train) if not Y_train[i]]\n",
    "\n",
    "# The histogram function can take a list of inputs and corresponding labels.\n",
    "plt.hist([text_lengths_pos, text_lengths_neg], bins=20, range=(0, 1000),\n",
    "         label=['positive', 'negative'])\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Also check the longest reviews.\n",
    "print('Longest positive review:', max(text_lengths_pos))\n",
    "print('Longest negative review:', max(text_lengths_neg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c3ZE9gpkml3a"
   },
   "source": [
    "---\n",
    "### Exercise 1: Token Counts (8 points)\n",
    "For each of the given tokens, construct a table with the number of positive training examples that include that token and the number of negative training examples that include that token. For reference, here are the counts for the first two tokens:\n",
    "\n",
    "|Token|Pos Count|Neg Count|\n",
    "|-|-|-|\n",
    "|good|4767|4849|\n",
    "|bad|1491|4396|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8YOYo6d01aWI"
   },
   "outputs": [],
   "source": [
    "tokens = ['good', 'bad', 'amazing', 'boring', 'laugh', 'cry']\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhzt-LnQ1m8w"
   },
   "source": [
    "## Feature Representation\n",
    "Consider the difference between the pixel features we used for image classification and the text features we are now dealing with.\n",
    "\n",
    "An image had 784 pixel positions. At each position, there is a single value in [0,1] (after normalization).\n",
    "\n",
    "In contrast, a review has a variable number of ordered tokens (up to 2494 in the training examples). Each token occurs in a particular position. We can think of the token positions much like the 784 pixel positions, except that some of the trailing positions are empty, since review lengths vary.  At each token position, there is a single token, one of the 88587 entries in the vocabulary. So we can think of a review as a (2500, 90000) matrix: At each of ~2500 token positions, we have 1 of ~90000 token ids.\n",
    "\n",
    "This representation would have 2500 * 90000 = 225 million features -- quite a lot more complexity than the images, though as you'll see below, we will make some simplifying assumptions, reducing both the number of token positions and the number of vocabulary items."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm_F5JmWyfko"
   },
   "source": [
    "### Padding and Reduced Length\n",
    "As is clear from the length histogram, the current representation of the review text is a variable-length array. Since fixed-length arrays are easier to work with in Tensorflow, let's add special padding tokens at the end of each review until they are all the same length.\n",
    "\n",
    "We'll also use this operation to limit the number of token positions by truncating all reviews to a specified length. In the code below, as an example, we pad all training inputs to length 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 766,
     "status": "ok",
     "timestamp": 1646684609342,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "a4ou8bSUCWOx",
    "outputId": "be4bc9e1-47aa-4f82-e23b-569f8d6128a4"
   },
   "outputs": [],
   "source": [
    "def pad_data(sequences, max_length):\n",
    "  # Keras has a convenient utility for padding a sequence.\n",
    "  # Also make sure we get a numpy array rather than an array of lists.\n",
    "  return np.array(list(\n",
    "      tf.keras.preprocessing.sequence.pad_sequences(\n",
    "          sequences, maxlen=max_length, padding='post', value=0)))\n",
    "\n",
    "# Pad and truncate to 300 tokens.\n",
    "X_train_padded = pad_data(X_train, max_length=300)\n",
    "\n",
    "# Check the padded output.\n",
    "print('Length of X_train[0]:', len(X_train[0]))\n",
    "print('Length of X_train_padded[0]:', len(X_train_padded[0]))\n",
    "print(X_train_padded[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFEmcwBjL4e_"
   },
   "source": [
    "### Reduced Vocabulary\n",
    "We also want to be able to limit the vocabulary size. Since our padding function produces fixed-length sequences in a numpy matrix, we can use clever numpy indexing to efficiently replace all token ids larger than some value with the designated out-of-vocabulary (OOV) id.\n",
    "\n",
    "In the code below, as an example, we'll keep just token ids less than 1000, replacing all others with OOV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 347,
     "status": "ok",
     "timestamp": 1646684634116,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "21qpyEgGNQeB",
    "outputId": "148ce890-4376-4c5a-f96a-9fb50cbe9538"
   },
   "outputs": [],
   "source": [
    "def limit_vocab(sequences, max_token_id, oov_id=2):\n",
    "  \"\"\"Replace token ids greater than or equal to max_token_id with the oov_id.\"\"\"\n",
    "  reduced_sequences = np.copy(sequences)\n",
    "  reduced_sequences[reduced_sequences >= max_token_id] = oov_id\n",
    "  return reduced_sequences\n",
    "\n",
    "# Reduce vocabulary to 1000 tokens.\n",
    "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
    "print(X_train_reduced[0])\n",
    "\n",
    "# Decode to see what this looks like in tokens. Note the '#' for OOVs.\n",
    "print(decode(X_train_reduced[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d24mOPC6ybC4"
   },
   "source": [
    "### One-hot Encoding\n",
    "Our current feature representations are **sparse**. That is, we only keep track of the token ids that are present in the input. A **one-hot** encoding replaces a value like 22 (corresponding to 'film') with an array with a single 1 at position 22 and zeros everywhere else. This will be very memory-inefficient, but we'll do it anyway for clarity.\n",
    "\n",
    "As discussed above, let's dramatically reduce both the number of token positions (review length) and the number of token ids (vocabulary). We'll clip each review after 20 tokens (so 2500 -> 20) and keep only the most common 1000 tokens (so 90000 -> 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1777,
     "status": "ok",
     "timestamp": 1646684668163,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "EXzkqVL3Jufj",
    "outputId": "69a06ea3-61e2-4d2b-ed57-69c51ee2c5f4"
   },
   "outputs": [],
   "source": [
    "# Keras has a util to create one-hot encodings.\n",
    "X_train_padded = pad_data(X_train, max_length=20)\n",
    "X_train_reduced = limit_vocab(X_train_padded, max_token_id=1000)\n",
    "X_train_one_hot = tf.keras.utils.to_categorical(X_train_reduced)\n",
    "print('X_train_one_hot shape:', X_train_one_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5RvIN4w66Ej"
   },
   "source": [
    "Note the shape of the one-hot encoded features. For each of our 25000 training examples, we have a 20 x 1000 matrix. That is, for each of 20 token positions, we have a vector of 1000 elements containing a single 1 and 999 zeros.\n",
    "\n",
    "We can think of these 1000-dimensional one-hot arrays as **embeddings**. Each token in the input has a 1000-dimensional representation. But because of the one-hot setup, the distance between each pair of tokens is the same ([1,0,0,...], [0,1,0,...], etc.). By contrast, learned embeddings result in meaningful distances between pairs of tokens. We'll get to that soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "296Cnt647b5c"
   },
   "source": [
    "## Logistic Regression with One-Hot Encodings\n",
    "Let's start with something familiar -- logistic regression. Since our feature representation is in 2 dimensions (20 x 1000), we need to flatten it to pass it to Keras (remember we did this with the pixel data too). Let's try two strategies for flattening.\n",
    "\n",
    "1. Flatten by *concatenating* (as we did with pixels), turning (20 x 1000) data into (20000,) data. The result is a separate feature for each token at each position.\n",
    "2. Flatten by *averaging* over token positions, turning (20 x 1000) data into (1000,) data. The result is an array with average token counts, ignoring position.\n",
    "\n",
    "NOTE: Our prior assignments have used the standard Stochastic Gradient Descent (SGD) optimizer to compute the gradient from an estimate of the loss (based on the current mini-batch). There are many alternative optimizers. Here we'll use the **Adam** optimizer, which sometimes gives better results. One key characteristic of Adam is that it effectively uses a different learning rate for each parameter rather than a fixed learning rate as in SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6m6eebM-0dUW"
   },
   "outputs": [],
   "source": [
    "def build_onehot_model(average_over_positions=False):\n",
    "  \"\"\"Build a tf.keras model for one-hot data.\"\"\"\n",
    "  # Clear session and remove randomness.\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  if average_over_positions:\n",
    "    # This layer averages over the first dimension of the input by default.\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "  else:\n",
    "    # Concatenate.\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      activation=\"sigmoid\"         # sigmoid activation for classification\n",
    "  ))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy',   # this is a classification task\n",
    "                optimizer='adam',             # fancy optimizer\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NY3W_1-OSZ2X"
   },
   "source": [
    "Now let's try fitting the model to our training data and check performance metrics on the validation (held-out) data. But first, here's a function for plotting the learning curves given the training history object we get from Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOVmajSuMjN6"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  plt.ylabel('Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.xticks(range(0, len(history['loss'] + 1)))\n",
    "  plt.plot(history['loss'], label=\"training\", marker='o')\n",
    "  plt.plot(history['val_loss'], label=\"validation\", marker='o')\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 22140,
     "status": "ok",
     "timestamp": 1646684718388,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "MyE4PgX70_op",
    "outputId": "3de05dfa-f372-4b77-ba43-f212c2f47c1e"
   },
   "outputs": [],
   "source": [
    "model = build_onehot_model()\n",
    "\n",
    "# Fit the model.\n",
    "history = model.fit(\n",
    "  x = X_train_one_hot,  # one-hot training data\n",
    "  y = Y_train,          # corresponding binary labels\n",
    "  epochs=5,             # number of passes through the training data\n",
    "  batch_size=64,        # mini-batch size\n",
    "  validation_split=0.1, # use a fraction of the examples for validation\n",
    "  verbose=1             # display some progress output during training\n",
    "  )\n",
    "\n",
    "# Convert the return value into a DataFrame so we can see the train loss \n",
    "# and binary accuracy after every epoch.\n",
    "history = pd.DataFrame(history.history)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuCh9aQPv7F_"
   },
   "source": [
    "---\n",
    "### Exercise 2: Comparing logistic regrerssion models (8 points)\n",
    "Train the one-hot model using both the concatenating and the averaging strategies and compare the results. Let's call these *LR-C* (Logistic Regression Concatenating) and *LR-A* (Logistic Regression Averaging). Then answer the following questions:\n",
    "\n",
    "1. What are the final training and validation accuracies for LR-C and LR-A?\n",
    "2. How many parameters are there in each model?\n",
    "3. Would you say that either model is overfitting? Why or why not?\n",
    "4. Briefly describe how LR-C differs from LR-A. How do you explain the relationship between their respective validation accuracy results? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CEAN5BejHc__"
   },
   "source": [
    "*Written answers:*\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJIBRqK7lsjG"
   },
   "source": [
    "## Logistic Regression with Embeddings\n",
    "Next, let's train model that replaces one-hot representations of each token with learned embeddings.\n",
    "\n",
    "The code below uses a Keras Embedding layer, which expects to receive a sparse (rather than one-hot) representation. That is, it expects a (padded) sequence of token ids; for each id, it looks up the corresponding embedding vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ho6uOeCaBs2e"
   },
   "outputs": [],
   "source": [
    "def build_embeddings_model(average_over_positions=False,\n",
    "                           vocab_size=1000,\n",
    "                           sequence_length=20,\n",
    "                           embedding_dim=2):\n",
    "  \"\"\"Build a tf.keras model using embeddings.\"\"\"\n",
    "  # Clear session and remove randomness.\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Embedding(\n",
    "      input_dim=vocab_size,\n",
    "      output_dim=embedding_dim,\n",
    "      input_length=sequence_length)\n",
    "  )\n",
    "\n",
    "  if average_over_positions:\n",
    "    # This layer averages over the first dimension of the input by default.\n",
    "    model.add(tf.keras.layers.GlobalAveragePooling1D())\n",
    "  else:\n",
    "    # Concatenate.\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      activation='sigmoid'         # apply the sigmoid function!\n",
    "  ))\n",
    "\n",
    "  model.compile(loss='binary_crossentropy', \n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyhoEjAiFSNB"
   },
   "source": [
    "Try training the model as before. We'll use the averaging strategy rather than the concatenating strategy for dealing with the token sequence. That is, we'll look up embedding vectors for each token. Then we'll average them to produce a single vector. Then we'll traing a logistic regression with that vector as input to predict the binary label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 5490,
     "status": "ok",
     "timestamp": 1646684762935,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "uYUE5UwkxoU8",
    "outputId": "20a4c237-8c49-4b24-9ea1-e0f325cb9908"
   },
   "outputs": [],
   "source": [
    "model = build_embeddings_model(average_over_positions=True,\n",
    "                               vocab_size=1000,\n",
    "                               sequence_length=20,\n",
    "                               embedding_dim=2)\n",
    "history = model.fit(\n",
    "  x = X_train_reduced,  # our sparse padded training data\n",
    "  y = Y_train,          # corresponding binary labels\n",
    "  epochs=5,             # number of passes through the training data\n",
    "  batch_size=64,        # mini-batch size\n",
    "  validation_split=0.1, # use a fraction of the examples for validation\n",
    "  verbose=1             # display some progress output during training\n",
    "  )\n",
    "\n",
    "history = pd.DataFrame(history.history)\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3k__61hFnag"
   },
   "source": [
    "---\n",
    "### Exercise 3: Experiments with embeddings (8 points)\n",
    "Train 6 models with embedding sizes in [2,4,8,16,32,64], keeping other settings fixed. Use the averaging strategy rather than the concatenating strategy.\n",
    "\n",
    "1. Construct a table with the training and validation accuracies of each model (after 5 training epochs).\n",
    "2. Compute the number of parameters in each model.\n",
    "3. Do learned embeddings appear to provide improved performance over the one-hot encoding? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7t46ZdX2ofd"
   },
   "source": [
    "*Written answers:*\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i2dWOuxqKHA6"
   },
   "source": [
    "## Inspecting Learned Embeddings\n",
    "Let's retrieve the learned embedding parameters from the trained model and plot the token embeddings.\n",
    "\n",
    "The model layers in a Keras Sequential model are stored as a list and the embeddings are the first layer. We can use the get_weights() function to get a numpy array with the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1646684774106,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "bfsbGSwkaFjo",
    "outputId": "d9d74723-012f-464f-a9f3-8a41d8178868"
   },
   "outputs": [],
   "source": [
    "# Display the model layers.\n",
    "display(model.layers)\n",
    "\n",
    "# Retrieve the embeddings layer, which itself is wrapped in a list.\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "display(embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apPWscNwcXTE"
   },
   "source": [
    "Now we'll use a fancy plotting tool called *plotly* to show the embeddings with hovertext so you can move your mouse over the points to see the corresponding tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "executionInfo": {
     "elapsed": 453,
     "status": "ok",
     "timestamp": 1646684778338,
     "user": {
      "displayName": "Daniel Gillick",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gg9DrSMlwYnG9EolecuJqe8n9m7fpcje4_UbYrhQ10=s64",
      "userId": "01872965353911650729"
     },
     "user_tz": 600
    },
    "id": "5RZMTrA0KttL",
    "outputId": "f5ec9b65-84bb-4c28-c673-cf86a87112df"
   },
   "outputs": [],
   "source": [
    "def plot_2d_embeddings(embeddings, id_start=1, count=100):\n",
    "  # Get 1st and 2nd embedding dims for the desired tokens.\n",
    "  x1 = embeddings[id_start:id_start+count, 0]\n",
    "  x2 = embeddings[id_start:id_start+count, 1]\n",
    "  \n",
    "  # Get the corresponding words from the reverse index (for labeling).\n",
    "  tokens = [reverse_index[i] for i in range(id_start, id_start+count)]\n",
    "\n",
    "  # Plot with the plotly library.\n",
    "  data = plotly.Scatter(x=x1, y=x2, text=tokens,\n",
    "                        mode='markers', textposition='bottom left',\n",
    "                        hoverinfo='text')\n",
    "  fig = plotly.Figure(data=[data],\n",
    "                      layout=plotly.Layout(title=\"Word Embeddings\",\n",
    "                                           hovermode='closest'))\n",
    "  fig.show()\n",
    "\n",
    "# Very frequent tokens tend to be more syntactic than semantic, so let's plot\n",
    "# some rarer words.    \n",
    "plot_2d_embeddings(embeddings, id_start=500, count=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z3Mm8MjRcZ20"
   },
   "source": [
    "---\n",
    "### Exercise 4: Interpretting Embeddings (8 points)\n",
    "Notice that the 2-D embeddings fall in a narrow diagonal band.\n",
    "\n",
    "1. Have the learned embeddings separated positive and negative words? What is the most negative word? Does this make sense?\n",
    "2. Give 2 examples of words that seem to have surprising embedding values and try to explain their positions. For example, what's going on with the tokens '7', '8', and '9'?\n",
    "3. The embedding for 'crazy' is very close to (0,0). Explain what this means in terms of the model's output.\n",
    "4. Can you explain what you think the 2 learned embedding dimensions mean, if anything?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x_qAAvvo2y3t"
   },
   "source": [
    "*Written answers:*\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. \n",
    "4. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXCitmUvxfwb"
   },
   "source": [
    "## Scaling Up!\n",
    "Remember how we limited our input sequences to 20 tokens and 1000 vocabulary entries? Let's see how well we can do using more data and bigger models (more parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKZDEGS7xzr6"
   },
   "source": [
    "### Exercise 5: Improve Results (8 points)\n",
    "Using pieces of code from above, set up and train a model that improves the validation accuracy to at least 80%. You should include the following elements:\n",
    "\n",
    "1. Truncate and pad input to the desired length.\n",
    "2. Limit vocabulary to the desired size.\n",
    "3. Set up a model using embeddings.\n",
    "4. Add an additional layer or layers (after the embeddings layer and before the output layer).\n",
    "5. Evaluate on the test data. Remember to apply the same pre-processing to the test data. You can use model.evaluate()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekbJ4sIq2hID"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7EEJI9yC2jOJ"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMbG/uKJC3itEUb58OjZyV3",
   "name": "09 Embeddings for Text.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
