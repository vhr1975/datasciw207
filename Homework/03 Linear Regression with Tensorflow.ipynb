{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Lab 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43534tdfgs-v"
   },
   "source": [
    "This lab continues our study of linear regression. You'll train your first models with Tensorflow, using a real dataset to predict car prices from their features. Note that Tensorflow is a rapidly changing library. This means you'll often see warnings about deprecations. You can ignore the warnings in our labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll use below.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHLcriKWLRe4"
   },
   "source": [
    "## Understanding the data\n",
    "Below, we'll train models using some real data. Here we'll use the [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile)  from 1985 Ward's Automotive Yearbook that is part of the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets).\n",
    "\n",
    "But before doing any training (or evaluating), let's make sure we understand what we're working with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_auto_data_set_text"
   },
   "source": [
    "### Load the data\n",
    "Load the data using the column names from [Automobile Data Set](https://archive.ics.uci.edu/ml/datasets/automobile). We'll only use a few of the columns so don't worry about understanding what they all mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_auto_data_set_code"
   },
   "outputs": [],
   "source": [
    "# Provide the names for the feature columns since the CSV file with the data\n",
    "# does not have a header row.\n",
    "cols = ['symboling', 'losses', 'make', 'fuel-type', 'aspiration', 'num-doors',\n",
    "        'body-style', 'drive-wheels', 'engine-location', 'wheel-base',\n",
    "        'length', 'width', 'height', 'weight', 'engine-type', 'num-cylinders',\n",
    "        'engine-size', 'fuel-system', 'bore', 'stroke', 'compression-ratio',\n",
    "        'horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
    "\n",
    "# Load the data from a CSV file into a pandas dataframe. Remember that each row\n",
    "# is an example and each column in a feature.\n",
    "car_data = pd.read_csv(\n",
    "    'https://storage.googleapis.com/ml_universities/cars_dataset/cars_data.csv',\n",
    "    sep=',', names=cols, header=None, encoding='latin-1')\n",
    "\n",
    "# Display applies built-in formatting for nicer printing, if available.\n",
    "display(car_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvcJJ_rUifF2"
   },
   "source": [
    "### Randomize\n",
    "Since we'll be using SGD (Stochastic Gradient Descent) for training, it is important that **each batch is a random sample of the data** so that the gradient computed is representative. Note that the original data (above) appears sorted by *make* in alphabetic order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3webN4USifuB"
   },
   "outputs": [],
   "source": [
    "# We want to shuffle the order of the rows without touching the columns.\n",
    "# First, we get a list of indices corresponding to the rows.\n",
    "indices = np.arange(car_data.shape[0])\n",
    "print('indices:', indices, '\\n')\n",
    "\n",
    "# Next, we shuffle the indices using np.random.permutation but set a random seed\n",
    "# so that everyone gets the same results each time.\n",
    "np.random.seed(0)\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "print('shuffled indices:', shuffled_indices, '\\n')\n",
    "\n",
    "# Finally, we use dataframe.reindex to change the ordering of the original\n",
    "# dataframe.\n",
    "car_data = car_data.reindex(shuffled_indices)\n",
    "display(car_data)\n",
    "\n",
    "# Note that this could be done in one fancy line:\n",
    "# car_data = car_data.reindex(np.random.permutation(car_data.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67gI95UG0FAW"
   },
   "source": [
    "### Feature selection\n",
    "To keep things simple, we will keep just a few of the 26 columns. Since the values come as strings, we need to convert them to floats. Also, we remove examples (rows) that have some missing value(s) of the columns we care about. Note that in general, there are various ways to deal with missing features, and this strategy of dropping examples with any missing feature is not ideal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zwu8udZY0Fkj"
   },
   "outputs": [],
   "source": [
    "# Choose a subset of columns (these are all numeric).\n",
    "columns = ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg', 'price']\n",
    "car_data = car_data[columns]\n",
    "\n",
    "# Convert strings to numeric values, coercing missing values to nan.\n",
    "for column in columns:\n",
    "  car_data[column] = pd.to_numeric(car_data[column], errors='coerce')\n",
    "\n",
    "# The dropna function drops rows with missing value(s) by default.\n",
    "car_data = car_data.dropna()\n",
    "\n",
    "# This leaves us with 199 examples.\n",
    "display(car_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S55LmJ9DwRJg"
   },
   "source": [
    "### Train/Test split\n",
    "Now that we've shuffled the order, we can split into portions for train and test easily. We'll try to avoid looking at the test data.\n",
    "\n",
    "We're going to train models that **predict price from the other columns**, so we'll create separate variables for input and output data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fj3U4nBMm0QX"
   },
   "outputs": [],
   "source": [
    "# We'll use these input features.\n",
    "features = ['horsepower', 'peak-rpm', 'city-mpg', 'highway-mpg']\n",
    "\n",
    "# Use a ~80/20 train/test split.\n",
    "car_train = car_data[:160]\n",
    "car_test = car_data[160:]\n",
    "\n",
    "# Create separate variables for features (inputs) and labels (outputs).\n",
    "# We will be using these in the cells below.\n",
    "car_train_features = car_train[features]\n",
    "car_test_features = car_test[features]\n",
    "car_train_labels = car_train['price']\n",
    "car_test_labels = car_test['price']\n",
    "\n",
    "# Confirm the data shapes are as expected.\n",
    "print('train data shape:', car_train_features.shape)\n",
    "print('train labels shape:', car_train_labels.shape)\n",
    "print('test data shape:', car_test_features.shape)\n",
    "print('test labels shape:', car_test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dGZgYo7gp4X"
   },
   "source": [
    "---\n",
    "### Exercise 1: Baseline (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have test data, we can evaluate a baseline. We'll use the average price of cars in the training set as our baseline model -- that is, the baseline always predicts the average price regardless of the input. And, instead of MSE, let's use **RMSE** (root mean squared error) -- that is, just take the square root of the MSE -- as our evaluation metric.\n",
    "\n",
    "1. Implement this baseline.\n",
    "2. Compute the RMSE of the baseline on both the train and test data.\n",
    "3. Is the test RMSE larger or smaller than the train RMSE? Explain whether this is what you'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "osBXeXWygp4T"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_j-BZ24fkWII"
   },
   "source": [
    "*Written answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dsBULSBygp4R"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wPQ2gQ4D8Jg6"
   },
   "source": [
    "### Feature histograms\n",
    "It's hard to stare at a matrix of 160x5 numbers (the shape of our training data) and know what to make of it. Plotting feature histograms is a good way to start building intuition about the data. This gives us a sense of the distribution of each feature, but not how the features relate to each other.\n",
    "\n",
    "We can also use the `describe` function to look at some aggregate statistics for the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BTz4yHT0xMUS"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 3))\n",
    "for i in range(len(columns)):\n",
    "  plt.subplot(1, 5, i+1)\n",
    "  plt.hist(np.array(car_train[columns[i]]))\n",
    "  plt.title(columns[i])\n",
    "plt.show()\n",
    "\n",
    "display(car_train.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4wvvzKoUIAN"
   },
   "source": [
    "---\n",
    "### Exercise 2: Feature correlations (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using pandas [`corr()`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to print all the pairwise correlation coefficients for the columns (use the training data only). See also the [Wikipedia page on correlation](https://en.wikipedia.org/wiki/Correlation) for more background.\n",
    "\n",
    "Then answer the following questions:\n",
    "\n",
    "1. It appears that higher-priced cars have higher or lower fuel efficiency?\n",
    "1. Which two features are likely to be most redundant?\n",
    "1. Which feature is likely to be least useful for predicting price?\n",
    "\n",
    "Extra (ungraded): try using [`sns.pairplot`](https://seaborn.pydata.org/generated/seaborn.pairplot.html) to examine each pair of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJtwrjdO6TbS"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9EH9D7Faf9n"
   },
   "source": [
    "*Written answer:*\n",
    "\n",
    "1. \n",
    "2. \n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxOhpvdW6TbX"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDsxLnljlp0C"
   },
   "source": [
    "## Tensorflow\n",
    "\n",
    "Let's train a linear regression model much like we did in the previous assignment, but this time using Tensorflow. Tensorflow is a powerful library with a complicated API so it is easy to get overwhelmed. We'll try to keep it simple."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjIwkSWBRp_i"
   },
   "source": [
    "### Build a model\n",
    "Here's how you use Tensorflow: First, you build a *computational graph*, and then you send data through it.\n",
    "\n",
    "This is confusing, but you'll get used to it. The computational graph for linear regression is very simple. There are many ways to build graphs, but the [Keras library](https://www.tensorflow.org/api_docs/python/tf/keras) is recommended. Here, we're using [`keras.layers.Dense`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) to create a model layer. We will go over Tensorflow and Keras in more detail, so don't worry about understanding everything now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfdRzjk-RgpG"
   },
   "outputs": [],
   "source": [
    "def build_model(num_features, learning_rate):\n",
    "  \"\"\"Build a TF linear regression model using Keras.\n",
    "\n",
    "  Args:\n",
    "    num_features: The number of input features.\n",
    "    learning_rate: The desired learning rate for SGD.\n",
    "\n",
    "  Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "  \"\"\"\n",
    "  # This is not strictly necessary, but each time you build a model, TF adds\n",
    "  # new nodes (rather than overwriting), so the colab session can end up\n",
    "  # storing lots of copies of the graph when you only care about the most\n",
    "  # recent. Also, as there is some randomness built into training with SGD,\n",
    "  # setting a random seed ensures that results are the same on each identical\n",
    "  # training run.\n",
    "  tf.keras.backend.clear_session()\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  # Build a model using keras.Sequential. While this is intended for neural\n",
    "  # networks (which may have multiple layers), we want just a single layer for\n",
    "  # linear regression.\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Dense(\n",
    "      units=1,                     # output dim\n",
    "      input_shape=[num_features],  # input dim\n",
    "      use_bias=True,               # use a bias (intercept) param\n",
    "      kernel_initializer=tf.ones_initializer,  # initialize params to 1\n",
    "      bias_initializer=tf.ones_initializer,    # initialize bias to 1\n",
    "  ))\n",
    "\n",
    "  # We need to choose an optimizer. We'll use SGD, which is actually mini-batch\n",
    "  # SGD. We can specify the batch size to use for training later.\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "  # Finally, we compile the model. This finalizes the graph for training.\n",
    "  # We specify the MSE loss.\n",
    "  model.compile(loss='mse', optimizer=optimizer)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oweNCtkrfSSm"
   },
   "source": [
    "After we've built a model, we can inspect the initial parameters (weights). There should be two ($w_0$ and $b$) and they should be initialized to 1, which we specified above using `tf.ones_initializer`. Unlike our code in the last assignment, Tensorflow stores the bias/intercept separately from the other weights/parameters for the layer. We also don't need to prepend a column of 1s to learn the bias -- Tensorflow handles this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "deD_nYrTfSvc"
   },
   "outputs": [],
   "source": [
    "# Build a model.\n",
    "model = build_model(num_features=1, learning_rate=0.0001)\n",
    "\n",
    "# Use get_weights() which returns lists of weights and biases for the layer.\n",
    "weights, biases = model.layers[0].get_weights()\n",
    "print('Weights:', weights)\n",
    "print('Biases:', biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V-XRH2zqhND9"
   },
   "source": [
    "Let's also try building a model with 2 features. Notice that in both cases, the weights are 2-D while the biases are 1-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ob6MerNohWjh"
   },
   "outputs": [],
   "source": [
    "# Build a model and look at the initial parameter values.\n",
    "model = build_model(num_features=2, learning_rate=0.0001)\n",
    "weights, biases = model.layers[0].get_weights()\n",
    "print('Weights:', weights)\n",
    "print('Biases:', biases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9sNzLTFWliB"
   },
   "source": [
    "### Making predictions\n",
    "Wait, we haven't trained yet! Why are we talking about making predictions? Well, remember that a model is a computational graph. That means we can pass data (of the expected shape) through the model (using the current values of the parameters) to make predictions.\n",
    "\n",
    "Before training, the parameters are set to their initial values (1s in our case). During training, we use the current predictions to compute a gradient and update the parameter values. Making predictions using the model without updating parameter values is called **Inference**.\n",
    "\n",
    "In the example code below, make sure you understand the output of `predict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrpMK0XfX4-M"
   },
   "outputs": [],
   "source": [
    "# Build a model that expects 1 input feature.\n",
    "model = build_model(num_features=1, learning_rate=0.0001)\n",
    "\n",
    "# Make a prediction for a single input.\n",
    "print(model.predict([42]))\n",
    "\n",
    "# Make predictions for 2 inputs.\n",
    "print(model.predict([42, 99]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymSX7q8zh9AJ"
   },
   "source": [
    "### Train a model\n",
    "Now let's actually train a model, initially with just 1 feature -- the horsepower. Notice that the `fit` function can take pandas DataFrame objects for input (x) and output (y). In addition, we can convert the return value into a DataFrame that tracks training metrics (in this case, training data loss and validation data loss) after each *epoch* (a full pass through the training data).\n",
    "\n",
    "Remember that we're using SGD, which is actually mini-batch SGD. That is, each time the model estimates the loss for the current weights, it randomly samples a batch of training examples (of the specified size) to do so.\n",
    "\n",
    "Finally, we'll reserve some more examples (taken out of the training set) as a *validation set*. We use this data to check for overfitting while training. Why not use the *test set* for this purpose? We want to maintain the purity of the test set so we try to only use it at the end of the experimental process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ixvnITrVf4r3"
   },
   "outputs": [],
   "source": [
    "model = build_model(num_features=1, learning_rate=0.0001)\n",
    "\n",
    "history = model.fit(\n",
    "  x = car_train_features[['horsepower']],\n",
    "  y = car_train_labels,\n",
    "  validation_split=0.1,  # use 10% of the examples as a validation set\n",
    "  epochs=5,\n",
    "  batch_size=32,\n",
    "  verbose=0)\n",
    "\n",
    "# Convert the return value into a DataFrame so we can see the loss after each\n",
    "# epoch. The history includes training data loss ('loss') and validation data\n",
    "# loss ('val_loss').\n",
    "history = pd.DataFrame(history.history)\n",
    "display(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3HOTSNmjKU5"
   },
   "source": [
    "### Feature scaling\n",
    "The loss is increasing as we train! What's going wrong?\n",
    "\n",
    "Look back at the histograms above. Notice that the scale of each feature value is different. Horsepower ranges from 48 to 262, while price ranges from \\$5118 to \\$45400. These different scales makes it more difficult to set the learning rate, and may make learning nearly impossible when we use multiple features (the scales of the gradients will overwhelm the actual feature importances).\n",
    "\n",
    "First, try reducing the learning rate above by 10x to 1e-5. That should fix the problem for now.\n",
    "\n",
    "But a better solution is to normalize the features so they are all roughly in the same range. We'll do this with mean and variance normalization. That is, for each feature, we subtract the mean (center the distribution on 0) and divide by the standard deviation (set the variance to 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_CdHB5ol6Gw"
   },
   "source": [
    "---\n",
    "### Exercise 3: Feature normalization (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply mean and variance normalization to produce car_train_features_norm and car_test_features_norm. These should be copies of the car_train_features and car_test_features, but with normalized feature values. Note that we're not normalizing the labels (prices).\n",
    "\n",
    "* DataFrame objects have `mean` and `std` functions you can use.\n",
    "* **Important:** You can't normalize the test data by computing mean and variance on the test data, as this would violate our willful blindness of the test data.\n",
    "* Use the `describe` function (as above) to verify your normalized data looks right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9KKnJ9ymfqc"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW0Kv3WLe1ZG"
   },
   "source": [
    "### Training with features\n",
    "We're ready to run some experiments with different sets of input features. To start, here's a simple function that plots train and validation set loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X7Bo1FKqe8B2"
   },
   "outputs": [],
   "source": [
    "def plot_loss(model, history):\n",
    "  \"\"\"Plot the loss after each training epoch.\"\"\"\n",
    "  # Convert the history object into a DataFrame.\n",
    "  history = pd.DataFrame(history.history)\n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.plot(range(len(history)), history['loss'], marker='.', color='black')\n",
    "  plt.plot(range(len(history)), history['val_loss'], marker='.', color='red')\n",
    "  plt.legend(['train loss', 'validation loss'])\n",
    "  plt.show()\n",
    "\n",
    "  # Show the final train loss value and the learned model weights.\n",
    "  print('Final train loss:', list(history['loss'])[-1])\n",
    "  print('Final weights:', model.layers[0].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5rwUw5FGae6"
   },
   "source": [
    "---\n",
    "### Exercise 4: Adjusting learning rate (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrain the model predicting price from horsepower, but now using your normalized features. Report validation loss for learning rates [0.0001, 0.001, 0.01, 0.1, 1] after 150 epochs of training. Which produces the best validation loss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iXUQ7dSHpKe4"
   },
   "outputs": [],
   "source": [
    "# EDIT CODE HERE\n",
    "model = build_model(num_features=1, learning_rate=.0001)\n",
    "\n",
    "history = model.fit(\n",
    "  # use the normalized features prepared above\n",
    "  x = car_train_features_norm[['horsepower']],\n",
    "  y = car_train_labels,\n",
    "  validation_split=0.1,\n",
    "  epochs=150,\n",
    "  batch_size=32,\n",
    "  verbose=0)\n",
    "\n",
    "plot_loss(model, history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6oPtykohZeP"
   },
   "source": [
    "*Written answer:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aNV4KM1QIBti"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JDXOSmd-h-rA"
   },
   "source": [
    "### Exercise 5: Adding features (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this exercise is to compile a table of results -- RMSE computed on the test data for the baseline and 4 models:\n",
    "1. features = horsepower\n",
    "2. features = horsepower, peak-rpm\n",
    "3. features = horsepower, peak-rpm, highway-mpg\n",
    "4. features = horsepower, peak-rpm, highway-mpg, city-mpg\n",
    "\n",
    "For consistency, use a batch size of 32, 150 epochs, and the best learning rate you found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Yarz9pGh-rC"
   },
   "outputs": [],
   "source": [
    "# EDIT CODE HERE\n",
    "def run_experiment(features, learning_rate):\n",
    "  model = build_model(len(features), learning_rate)\n",
    "\n",
    "  history = model.fit(\n",
    "    x = car_train_features_norm[features],\n",
    "    y = car_train_labels,\n",
    "    validation_split=0.1,\n",
    "    epochs=150,\n",
    "    batch_size=32,\n",
    "    verbose=0)\n",
    "\n",
    "  plot_loss(model, history)\n",
    "\n",
    "  # Make predictions on test data\n",
    "  test_loss = model.evaluate(car_test_features_norm[features],\n",
    "                             car_test_labels,\n",
    "                             verbose=0)\n",
    "  test_rmse = np.sqrt(test_loss)\n",
    "  print('Test rmse:', test_rmse)\n",
    "\n",
    "run_experiment(features, 0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBcyy0oTh-rF"
   },
   "source": [
    "*Written answer:*\n",
    "\n",
    "Model | Test RMSE\n",
    "--- | ---\n",
    "Baseline | \n",
    "Horsepower | \n",
    "  + Peak-RPM | \n",
    "  + Highway-MPG | \n",
    "  + City-MPG | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNWpEPgZjyNL"
   },
   "source": [
    "## Review\n",
    "* The **[Pandas](https://pandas.pydata.org/) library** is very useful for manipulating datasets and works well with numpy.\n",
    "* Use a random split into train and test data and measure performance on the test data, starting from a simple **baseline**.\n",
    "* Examine data using histograms and correlations to help build intuition before training any models.\n",
    "* **Tensorflow** works by first building a **computational graph**; then, you can pass data through the graph to produce predictions, updating parameters via gradient descent in training mode; we use the **Keras API** to easily configure models.\n",
    "* Training is often quite sensitive to the **learning rate** hyperparameter, and feature normalization is an important strategy to avoid differences in the scale of the feature derivatives (gradient) that can make learning impossible."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "copyright"
   ],
   "name": "03 Linear Regression with Tensorflow.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
