{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKsRDH5ZUdfasdv"
   },
   "source": [
    "# Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43534tdfgs-v"
   },
   "source": [
    "In this lab we will train classifiers for images using [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) and Tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7X58hOMTUH-w"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we'll use below.\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns  # for nicer plots\n",
    "sns.set(style=\"darkgrid\")  # default style\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import metrics\n",
    "tf.get_logger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHLcriKWLRe4"
   },
   "source": [
    "## Understanding the data\n",
    "We'll train models on the [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist) dataset. This consists of 70,000 grayscale images (28x28). Each image is associated with 1 of 10 classes. The dataset was split by the creators; there are 60,000 training images and 10,000 test images. \n",
    "\n",
    "Fashion MNIST classes:\n",
    "* T-shirt/top\n",
    "* Trouser\n",
    "* Pullover\n",
    "* Dress\n",
    "* Coat\n",
    "* Sandal\n",
    "* Shirt\n",
    "* Sneaker\n",
    "* Bag\n",
    "* Ankle boot\n",
    "\n",
    "Before doing any training (or evaluating), let's make sure we understand what we're working with. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "load_auto_data_set_text"
   },
   "source": [
    "### Load the data\n",
    "Tensorflow includes a growing [library of datasets](https://www.tensorflow.org/datasets/catalog/overview) and makes it easy to load them in numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_auto_data_set_code"
   },
   "outputs": [],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "# Load the Fashion MNIST dataset.\n",
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Flatten Y_train and Y_test, so they become vectors of label values.\n",
    "# The label for X_train[0] is in Y_train[0].\n",
    "Y_train = Y_train.flatten()\n",
    "Y_test = Y_test.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Eg9mUz9zcIil"
   },
   "source": [
    "### Basic Analysis\n",
    "Notice that `X_train`, `Y_train`, `X_test`, and `Y_test` are all numpy arrays. Let's print their shapes to confirm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6jGmsYAdKEK"
   },
   "outputs": [],
   "source": [
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"Y_train.shape:\", Y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"Y_test.shape:\", Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDeim44-dler"
   },
   "source": [
    "Notice that there are 60,000 instances in `X_train`. Each of these is a grayscale image represented by an 28-by-28 array of grayscale pixel values between 0 and 255 (the larger the value, the lighter the pixel). Before we continue, let's apply linear scaling to our pixel values, so they all fall between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXukkWBWdlGj"
   },
   "outputs": [],
   "source": [
    "# Pixel values range from 0 to 255. To normalize the data, \n",
    "# we just need to divide all values by 255.\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWsOQ3tbCOA7"
   },
   "source": [
    "In the previous lab, our input data had just a few features. Here, we treat **every pixel value as a separate feature**, so each input example has 28x28 (784) features!\n",
    "\n",
    "Fashion MNIST images have one of 10 possible labels (shown above). Since the labels are indices 0-9, let's keep a list of (string) names for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZpFxxStzSrP"
   },
   "outputs": [],
   "source": [
    "label_names = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5aXeK2VznCW"
   },
   "source": [
    "Next let's use the `imshow` function to look at the first few images in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7mfV9b4I7cMW"
   },
   "outputs": [],
   "source": [
    "# Create a figure with subplots. This returns a list of object handles in axs\n",
    "# which we can use to populate the plots.\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(10,5))\n",
    "for i in range(5):\n",
    "  image = X_train[i]\n",
    "  label = Y_train[i]\n",
    "  label_name = label_names[label]\n",
    "  axs[i].imshow(image, cmap='gray')\n",
    "  axs[i].set_title(label_name)\n",
    "  axs[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aFx7Ts3b6N6d"
   },
   "source": [
    "---\n",
    "### Exercise 1 (8 points)\n",
    "\n",
    "Display the first 5 images for each class in a 10x5 grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJtwrjdO6TbS"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxOhpvdW6TbX"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qNvSx5tybqMR"
   },
   "source": [
    "## Sneaker Classification\n",
    "There are many things we can do with this dataset. Following our lectures, let's start with binary classification. We'll train a sneaker classifier, using sneaker images (class 7) as our positive examples ($y=1$) and all other images as negative examples ($y=0$).\n",
    "\n",
    "Once we've trained a model, it will produce predictions $\\hat{y}$, the probability that an input image $x$ is a sneaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DvcJJ_rUifF2"
   },
   "source": [
    "### Data Preprocessing\n",
    "Before we continue, we need to prepare the data for our binary classification task. The label for all the sneaker images should be 1 and the label for all the non-sneaker images should be 0. \n",
    "\n",
    "Programming note: Numpy allows us to perform what is called [boolean array indexing](https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#boolean-array-indexing).\n",
    "This means that a numpy array $A$ can be be modified according to boolean conditions in a matching sized array $B$ \n",
    "as follows: $A[B]$.\n",
    "We use this indexing to efficienty set our binary labels below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3webN4USifuB"
   },
   "outputs": [],
   "source": [
    "# Make copies of the original dataset for binary classification task.\n",
    "X_train_binary = np.copy(X_train)\n",
    "X_test_binary = np.copy(X_test)\n",
    "Y_train_binary = np.copy(Y_train)\n",
    "Y_test_binary = np.copy(Y_test)\n",
    "\n",
    "# Set labels: 1 for sneaker images, 0 for the others.\n",
    "# Note that a boolean array is created when Y_train_binary != 7 is evaluated.\n",
    "Y_train_binary[Y_train_binary != 7] = 0.0 \n",
    "Y_train_binary[Y_train_binary == 7] = 1.0\n",
    "Y_test_binary[Y_test_binary != 7] = 0.0\n",
    "Y_test_binary[Y_test_binary == 7] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84idQ2_Mm8w-"
   },
   "source": [
    "---\n",
    "### Exercise 2 (8 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Before training any models, let's work some more with our raw feature values, comparing sneaker and non-sneaker images.\n",
    "\n",
    "1. Find all sneaker images in `X_train_binary` and compute the mean and standard deviation of the center pixel across all sneaker images. The center pixel is located at position [14, 14].\n",
    "\n",
    "2.   Find all non-sneaker images in `X_train_binary` and compute the mean and standard deviation of the center pixel  across all non-sneaker images. The center pixel is located at position [14, 14].\n",
    "\n",
    "3. Repeat 1 and 2 for the pixel located at positon [3, 14].\n",
    "\n",
    "4. Based on your results, do you think there is some evidence that suggests that we can use pixel values to discriminate between sneaker and non-sneaker images? Justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FblS-OVmns4a"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (1-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j9EH9D7Faf9n"
   },
   "source": [
    "*Written answer:*\n",
    "\n",
    "1.\n",
    "\n",
    "2.\n",
    "\n",
    "3.\n",
    "\n",
    "4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D35qqLaPd1jk"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br_TAXD7p5gQ"
   },
   "source": [
    "### Data Shuffling\n",
    "Just like in the previous lab, we'll be using SGD (Stochastic Gradient Descent) for training. This means that it is important that **each batch is a random sample of the data**.\n",
    "\n",
    "This time we can use [integer array indexing](https://numpy.org/doc/stable/reference/arrays.indexing.html#integer-array-indexing) to re-order the data and labels using a list of shuffled indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sNryj_kGq8vV"
   },
   "outputs": [],
   "source": [
    "np.random.seed(0) # For reproducibility\n",
    "\n",
    "indices = np.arange(X_train_binary.shape[0])\n",
    "shuffled_indices = np.random.permutation(indices)\n",
    "\n",
    "# Re-order training examples and corresponding labels using the randomly\n",
    "# shuffled indices.\n",
    "X_train_binary = X_train_binary[shuffled_indices]\n",
    "Y_train_binary = Y_train_binary[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E01QY9SRpwPE"
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Recall that logistic regression is an application of the logistic (sigmoid) function to the linear regression model:\n",
    "\n",
    "\\begin{equation}\n",
    "y=\\frac{1}{1+e^{-z}} \n",
    "\\end{equation}\n",
    "\n",
    "<center>\n",
    "<img src=\"https://developers.google.com/machine-learning/crash-course/images/SigmoidFunction.png\" alt=\"Sigmoid Function\" width=\"45%\"/>\n",
    "</center>\n",
    "\n",
    "where:\n",
    "\n",
    "\\begin{equation}\n",
    "z = b + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "\\end{equation}\n",
    "\n",
    "As you can see, the output $y$ is in the range $[0,1]$, the probability of the positive class. If we want a label (instead of a probability), we need to choose a threshold, like $0.5$. A value above or equal to that threshold indicates that the input should be classified as positive (e.g., sneaker); a value below the threshold indicates that the input should be classified as negative (e.g. non-sneaker)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PsfzNnVFwh0K"
   },
   "source": [
    "### Loss Function\n",
    "\n",
    "In place of MSE, which we used for linear regression, we need a loss function for logistic regression that's appropriate for classification. The *Log Loss* (also known as *binary cross-entropy*), is defined as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "-\\frac{1}{|Y|} \\sum_{y_i \\in Y}y_i log(\\hat{y}_i) + (1−y_i)log(1−\\hat{y}_i)\n",
    "\\end{equation}\n",
    "\n",
    "Recall that $y_i$ is the label for example $i$ and $\\hat{y_i}$ is the predicted probability (of the positive class) for example $i$. Note that only the first term in the sum is active for positive examples (the second term is 0 when $y_i = 1$) and only the second term in the sum is active for negative examples (the first term is 0 when $y_i = 0$).\n",
    "\n",
    "The log loss is differentiable, allowing us to compute gradients and run SGD. It also happens to be convex, which guarantees that SGD (with a suitable learning rate) will produce a global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGgQ-ASYiHbK"
   },
   "source": [
    "## Baseline\n",
    "\n",
    "When dealing with classification problems, a simple, but useful baseline is to select the *majority* class (the most common label in the training set) and use it as the prediction for all inputs.\n",
    "\n",
    "Our training dataset consists of 6,000 sneaker examples (10%), and 54,000 non-sneaker images (90%). So our majority class baseline classifies everything as *non-sneaker*. Notice that, for our particular dataset, this will yield an accuracy of 90%. Let's see if we can train a model that can beat the baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ls30LfwsIZCv"
   },
   "outputs": [],
   "source": [
    "print(\"Number of sneaker images in training set: %d\"%(Y_train_binary == 1).sum())\n",
    "print(\"Number of non-sneaker images in training set: %d\"%(Y_train_binary == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXNj1z0IlDw_"
   },
   "source": [
    "---\n",
    "### Exercise 3 (8 points)\n",
    "\n",
    "Implement a function that computes the Log Loss (binary cross-entropy) metric and use it to evaluate our baseline on both the train and test data. Use 0.1 as the predicted probability for your baseline (reflecting what we know about the original distribution of classes in our dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oFM62C3uhoq_"
   },
   "outputs": [],
   "source": [
    "def log_loss(Y_true, Y_pred):\n",
    "  \"\"\"Returns the binary log loss for a list of labels and predictions.\n",
    "  \n",
    "  Args:\n",
    "    Y_true: A list of (true) labels (0 or 1)\n",
    "    Y_pred: A list of corresponding predicted probabilities\n",
    "\n",
    "  Returns:\n",
    "    Binary log loss\n",
    "  \"\"\"\n",
    "  # YOUR CODE HERE\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZNKo4axhzY4"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "35A7So50uc-3"
   },
   "source": [
    "## Build a model\n",
    "We will use Tensorflow/Keras to build our logistic regression model. This should look very similar to the models you built for linear regression but with a few key differences:\n",
    "* We use the Keras flatten layer to turn the 2-D 28x28 pixel grid inputs into 1-D vector inputs.\n",
    "* We configure our dense layer with a sigmoid activation, which applies a sigmoid function to the output of the linear mapping $xW^T$.\n",
    "* We specify binary_crossentropy as the loss (synonymous with log loss) when compiling the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eaKF1dVEwsJq"
   },
   "outputs": [],
   "source": [
    "def build_model(learning_rate=0.01):\n",
    "  \"\"\"Build a TF logistic regression model using Keras.\n",
    "\n",
    "  Args:\n",
    "    learning_rate: The desired learning rate for SGD.\n",
    "\n",
    "  Returns:\n",
    "    model: A tf.keras model (graph).\n",
    "  \"\"\"\n",
    "  # This is not strictly necessary, but each time you build a model, TF adds\n",
    "  # new nodes (rather than overwriting), so the colab session can end up\n",
    "  # storing lots of copies of the graph when you only care about the most\n",
    "  # recent. Also, as there is some randomness built into training with SGD,\n",
    "  # setting a random seed ensures that results are the same on each identical\n",
    "  # training run.\n",
    "  tf.keras.backend.clear_session()\n",
    "  np.random.seed(0)\n",
    "  tf.random.set_seed(0)\n",
    "\n",
    "  # Build a model using keras.Sequential.\n",
    "  model = keras.Sequential()\n",
    "\n",
    "  # Keras layers can do pre-processing. This layer will take our 28x28 images\n",
    "  # and flatten them into vectors of size 784.\n",
    "  model.add(keras.layers.Flatten())\n",
    "  \n",
    "  # This layer constructs the linear set of parameters for each input feature\n",
    "  # (as well as a bias), and applies a sigmoid to the result. The result is\n",
    "  # binary logistic regression.\n",
    "  model.add(keras.layers.Dense(\n",
    "      units=1,                     # output dim (for binary classification)\n",
    "      use_bias=True,               # use a bias param\n",
    "      activation=\"sigmoid\"         # apply the sigmoid function!\n",
    "  ))\n",
    "\n",
    "  # Use the SGD optimizer as usual.\n",
    "  optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)\n",
    "\n",
    "  # We specify the binary_crossentropy loss (equivalent to log loss).\n",
    "  # Notice that we are including 'binary accuracy' as one of the metrics that we\n",
    "  # ask Tensorflow to report when evaluating the model.\n",
    "  model.compile(loss='binary_crossentropy', \n",
    "                optimizer=optimizer, \n",
    "                metrics=[metrics.binary_accuracy])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e9sNzLTFWliB"
   },
   "source": [
    "Let's make sure model building code works. Before training, the parameters of the model are initialized randomly (this is the default). While the untrained model won't make good predictions, we should still be able to pass data through it and get probability outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrpMK0XfX4-M"
   },
   "outputs": [],
   "source": [
    "# Build the model.\n",
    "model = build_model()\n",
    "\n",
    "# Make a prediction for five inputs.\n",
    "print(model.predict(X_train_binary[0:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymQ3i8aRiCcO"
   },
   "source": [
    "As expected, the outputs look like probabilities (in [0,1]). Once the model is trained, we hope that these predictions correspond to the probability that each input image is a sneaker."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OUQDiYqCh2Ns"
   },
   "source": [
    "## Train a model\n",
    "Let's train the model. Note that we're using 10% of the training data as a *validation split*. This serves a similar purpose to our test data, allowing us to check for over-fitting during training. We don't use the test data here because we might run lots of experiments, and over time, we might adjust settings to improve results on the validation set. We want to preserve the purity of the test data to allow for the cleanest possible evaluation at the end of the experimentation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KkUagTzxpWdr"
   },
   "outputs": [],
   "source": [
    "model = build_model(learning_rate=0.01)\n",
    "\n",
    "# Fit the model.\n",
    "history = model.fit(\n",
    "  x = X_train_binary,   # our binary training examples\n",
    "  y = Y_train_binary,   # corresponding binary labels\n",
    "  epochs=5,             # number of passes through the training data\n",
    "  batch_size=64,        # mini-batch size for SGD\n",
    "  validation_split=0.1, # use a fraction of the examples for validation\n",
    "  verbose=1             # display some progress output during training\n",
    "  )\n",
    "\n",
    "# Convert the return value into a DataFrame so we can see the train loss \n",
    "# and binary accuracy after every epoch.\n",
    "history = pd.DataFrame(history.history)\n",
    "display(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eS8lOgCK-0At"
   },
   "source": [
    "## Evaluate the model\n",
    "\n",
    "Good news. It appears that our model is doing better than our baseline. Let's use the trained model to predict probabilities for the test data. We can use `predict` to run *inference*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t0PU3br4-zv8"
   },
   "outputs": [],
   "source": [
    "# The result of model.predict has an extra dimension, so we flatten to get a\n",
    "# vector of predictions. Note that these are the predicted probabilities of the\n",
    "# positive (sneaker) class.\n",
    "test_predictions = model.predict(X_test_binary).flatten()\n",
    "print(test_predictions.shape)\n",
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LTdRTsyM2YvI"
   },
   "source": [
    "---\n",
    "### Exercise 4 (8 points)\n",
    "\n",
    "Compute the accuracy on the test data using a threshold of 0.5. Remember to use Y_test_binary for the true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-o6Bdkl2gQT"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LVEF6i092hfl"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV_gTk-9YI3d"
   },
   "source": [
    "## Analyze the model\n",
    "\n",
    "Let's investigate what the model has learned. Recall how to get the learned weights from the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9aw-V-FTlaq"
   },
   "outputs": [],
   "source": [
    "# The model includes 2 layers: a flattening layer and a dense layer.\n",
    "print(model.layers)\n",
    "\n",
    "# Retrieve the weights and biases from the dense layer.\n",
    "weights, biases = model.layers[1].get_weights()\n",
    "bias = biases[0]  # there's only 1 bias\n",
    "weights = weights.flatten()  # flatten the weights to a vector\n",
    "print('Bias:', bias)\n",
    "print('Weights shape:', weights.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gfq7t1naoTM"
   },
   "source": [
    "---\n",
    "### Exercise 5 (8 points)\n",
    "\n",
    "1. Using the your knowledge of the model's functional form, compute the predicted probability (of the sneaker class) for an image of all 0-valued inputs.\n",
    "\n",
    "2. Confirm this probability by constructing a fake image with all 0s and use model.predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3d4NzODhbB8G"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LsCGfuB1c_vs"
   },
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "copyright",
    "LOcjWKAljbqr",
    "bCBT54r7k2YL",
    "loqjuOZFlEdt"
   ],
   "name": "04 Logistic Regression with Tensorflow.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
